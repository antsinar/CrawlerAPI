# Web Crawler

Ο web crawler, ή από εδώ και πέρα αράχνη, είναι η βάση του όλου πρότζεκτ και από όπου ξεκίνησε η όλη ιδέα για το παιχνίδι. Μία πρώτη υλοποίηση μπορεί να βρεθεί στο repository [antsinar/dayProjects](https://github.com/antsinar/dayProjects)

Χωρίζεται σε δύο βασικά modules, την αράχνη καθ'αυτή, που τρέχει τη διαδικασία ανάκτησης σελίδων από το domain στόχό, και τον processor, που αναλαμβάνει τη καταχώρηση και τη προώθηση εργασιών προς εκτέλεση. 

## Αντιμετώπιση Ιστοσελίδας Στόχου

## Σχεδιαστικές Επιλογές
Ο σχεδιασμός του module έπρεπε σε πρώτη φάση να υπακούει στα 3 ακόλουθα στοιχεία:

1. Να είναι επεκτάσιμο και αντικαταστήσιμο
2. Να τρέχει εντός του process του webserver
3. Να μην επιβαρύνει το async event loop και να μην αφήνει σφάλματα στο runtime

Για το πρώτο σημείο εφαρμόστηκε μια decoupled λογική. Η αράχνη δεν εκτελείται έως ότου λάβει μια εργασία από τον processor. Ο processor με τη σειρά του έχει το ρόλο να λαμβάνει τα αιτήματα του χρήστη και να περιορίζει πόσες (παραμετροποιημένες) εκδόσεις της αράχνης εκτελούνται με βάση configuration. Οι εκδοχές των δύο modules χρησιμοποιούν wrapper συναρτήσεις για να καλύψουν λειτουργικές λεπτομέριες, οπότε και θα είναι σχετικά ανώδυνο να αντικατασταθούν στο μέλλον.

Στο δεύτερο σημείο έπρεπε να συμορφωθούμε στο τεχνιτό περιορισμό των δυνατοτήτων της εφαρμογής. Με τη χρήση εξωτερικής ουράς μηνυμάτων (RabbitMQ, Redis κτλ) θα μπορούσαμε τρέξουμε περισσότερα tasks αράχνης που να είναι πραγματικά ανεξάρτητα μεταξύ τους. Παρόλα αυτά η ουρά διαχείρησης των εργασιών θα παραμείνει in-memory ώστε να μην υπάρχει το κόστος και το overhead ενός ακόμη service. Χάνετε δηλαδή προσωρινά η δυνατότητα πραγματικά παράλληλης επεξεργασίας, άρα και κλιμάκωσης της εφαρμογής, σε αντάλλαγμα για ένα πιο άμεσο και οικονομικό λειτουργικό πρωτότυπο.

Το τρίτο σημείο, σε συνδυασμό με το δεύτερο, ήταν το πιο ιδιαίτερο στην επίλυση. 
Χρονολογικά, ένα url εισέρχεται στην ουρά μηνυμάτων του processor ώστε να δημιουργηθεί μια εργασία. **Πως παρακολουθούμε όμως την ουρά μηνυμάτων για νέα μηνύματα;** 
Έπειτα, η εργασία καλείτε με ασύγχρονο τρόπο, χωρίς να περιμένουμε απαραίτητα ένα επιτυχημένο αποτέλεσμα. **Πως διασφαλίζουμε πως ένα task που υπάρχει απλά στο runtime δε θα προκαλέσει σφάλματα και στην υπόλοιπη εφαρμογή, από τη στιγμή που το περιβάλλον εκτέλεσης μοιράζεται;** Και τέλος, μπορεί μια αράχνη να είναι μια κατά βάση I/O διαδικασία, λόγω συνδέσεων στο δίκτυο, όμως η επεξερασία html εγγράφων και εξαγωγή link από αυτούς δεν είναι. **Πως περιορίζεται η κατανάλωση πόρων επεξεργαστή και επομένως οι blocking διαδικασίες εντός του event loop;** Μια σύντομη απάντηση θα ήταν με τη χρήση γρήγορων compiled βιβλιοθηκών και έξυπνη αναζήτηση εντός του εγγράφου, όμως περισσότερες λεπτομέριες υπάρχουν παρακάτω στην υλοποίηση της αράχνης.

### Στρατηγική Εξερεύνησης
Τη δεδομένη στιγμή που γράφεται αυτή η περιγραφή, ο αλγόριθμος που διατρέχει κάθε site θα μπορούσε να χαρακτηριστεί ως greedy/eager. Και αυτό διότι αποφασίζει πια σελίδα να επισκευθεί οπορτουνιστικά, δηλαδή την πρώτη διαθέσιμη. Αυτό δεν αποτελεί optimal στρατηγική και χρήση των διαθέσιμων resources, είναι όμως μια βάση που λειτουργεί.

...

### Υλοποίηση Αράχνης
Η λογική του αλγορίθμου της αράχνης ενσωματώνεται στη κλάση `Crawler`, ενώ υπάρχουν και βοηθητικές συναρτήσεις `generate_client` & `process_url`.
Ξεκινώντας από το τέλος, η συνάρτηση `generate_client`, τύπου ασύγχρονης γεννήτριας, δημιουργεί έναν ασύχρονο http client, χρησιμοποιώντας τις βιβλιοθήκες httpx και contextlib (standard library), ενώ έχει και την αρμοδιότητα διαχείρισης τυχών http λαθών.

```py title="src/lib.py" linenums="1"
@asynccontextmanager
async def generate_client(
    base_url: Optional[str] = "",
) -> AsyncGenerator[AsyncClient, None]:
    """Configure an async http client for the crawler to use"""
    headers = {
        "User-Agent": "MapMakingCrawler/x.y.z",
        "Accept": "text/html,application/json,application/xml;q=0.9",
        "Keep-Alive": "500",
        "Connection": "keep-alive",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept-Language": "en, el-GR;q=0.9",
    }
    transport = AsyncHTTPTransport(retries=3, http2=True)
    client = AsyncClient(
        base_url=base_url,
        transport=transport,
        headers=headers,
        follow_redirects=True,
        default_encoding=lambda content: chardet.detect(content).get("encoding"),
    )
    try:
        yield client
    except RequestError as e:
        logger.error(e)
    finally:
        await client.aclose()
```
Από πάνω προς τα κάτω, ξεκινάμε ορίζοντας τη μέθοδο με τον decorator `@asynccontextmanager` ώστε να μπορεί να χρησιμοποιηθεί με το keyword `async with` μέσα σε άλλα μπλοκ κώδικα.

Η παράμετρος `base_url` είναι προαιρετική και θέτει ένα σταθερό string στον http client που συνήθως είναι το domain name του site στόχου.

Στη συνέχεια υπάρχει μια λίστα με headers που πρέπει να υπάρχουν ώστε να απαντήσει σωστά ο web server του site στόχου. Τα headers `Keep-Alive` & `Connection` απορρίπτονται όταν χρησιμοποιείται πρωτόκολο http/2, όπως θα δούμε αργότερα. Για την υποστήριξη του brotli αλγορίθμου συμπίεσης, όπως φαίνεται στο header `Accept-Encoding` χρειάστηκε η εγκατάσταση του πακέτου [brotli](https://github.com/google/brotli).

Αργότερα ορίζεται ένα [HTTPTransport Layer](https://www.python-httpx.org/advanced/transports/) ώστε να ενσωματωθεί η δυνατότητα επανάληψης ανεπιτυχών requests, αλλά και υποστήριξη http/2 προτοκόλου, μέσω της βιβλιοθήκης [h2](https://python-hyper.org/projects/hyper-h2/en/stable/). Το προτόκολο http/2 χρησιμοποιείται ώστε να μη ξεχωρίζει τόσο πολύ η αράχνη από κοινούς επισκέπτες του site στόχου. Θεωρητικά, δε θα πρέπει να υπάρχει κάποιο κέρδος στην απόδοση, μιας και δεν αξιοποιείται κάπου η [δυνατότητα αποστολής συγκεντροτικών TCP συνδέσεων](https://www.akamai.com/blog/performance/improve-ux-with-http2-multiplexed-requests). 

Ο http client παίρνει, πέρα του `base_url` `headers` και `transport`, τις παραμέτρους `follow_redirects` και `default_encoding`. Το `follow_redirects` προορίζεται για τις 3XX responses, ενώ το `default_encoding` για τη κωδικοποίηση του περιεχομένου εντός του html αρχείου. Αυτό γίνεται αυτόματα με τη χρήση του πακέτου [chardet](https://github.com/chardet/chardet).

Τέλος η συνάρτηση επιστρέφει μέσω yield τον http client, ενώ φροντίζει να κλείσει τη διεργασία με το keyword `finally`. 

### Σημειώσεις: Αράχνη
#### Εξερεύνηση
    
περιγραφή

#### DNS Caching & Transport Layer

περιγραφή

### Στρατηγική Δημιουργίας Εργασιών 

### Υλοποίηση Δημιουργίας Εργασιών

### Σημειώσεις: Εργασίες Αράχνης
#### Παρακολούθηση Ουράς

περιγραφή

## Μετρικές Απόδοσης

* Ποσοστό σελίδων που βρέθηκαν
* Peak & baseline χρήση μνήμης σε αναλογία με το μέγεθος του γράφου
* Peak & baseline χρήση επεξεργαστή σε αναλογία με το μέγεθος του γράφου

## Πηγές

* [HTTP/2 For Web Developers - Cloudflare](https://blog.cloudflare.com/http-2-for-web-developers/)
* [Improve User Experience with Parallel Execution of HTTP/2 Multiplexed Requests - Akamai](https://www.akamai.com/blog/performance/improve-ux-with-http2-multiplexed-requests)